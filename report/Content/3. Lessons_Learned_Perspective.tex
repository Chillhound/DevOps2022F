\section{Lessons learned perspective}
The following problems taught the lesson of taking the time to do things right the first time (or at least when you recognize that a problem exists) is invaluable compared to continue working with incomplete and ineffective workarounds. e.g. Being eager to finish tasks fast and thus not prioritizing attention to important details.

\subsection{Hacking directly on the server}
For quite some time, until \href{https://github.com/Chillhound/DevOps2022F/commit/e45526bdfac81a342d1c74413dfd561e0bf05a89}{commit e455}, our CI pipeline was not correctly set up resulting in manual labor being necessary when we were to deploy changes. The misconfiguration stemmed from 
the pipeline not updating the Docker Compose file on the droplet and as such we had to manually stop all containers, remove images and then run the docker-compose file. \\
Doing this manual task is not hard in itself but problems arose when things did not work in the first go. In these scenarios group members would take the path of least resistance and make changes to e.g. the Docker setup directly on the server making it hard to track which changes actually worked and then correctly adding them to version control afterwards. A problem that we faced because of this was one time where we actually lost a working change because we got confused about which changes made where had resulted in the system working correctly. \\
This happened when we decided to try and make our database not publicly exposed (discussed in the next section). We are not entirely sure what happened, but it had something to do with us manually changing configurations directly on the server, not correctly committing the same changes to version control and on top of that, not knowing that the CI pipeline was not correctly configured to run the newest Docker images. \\
This really drove home the lesson that automation, configuration and reproduceability beats manual work, because it is too easy to forget something and not be able to get repeated results.

\subsection{Attacks on database and migrating to cloud}
As mentioned in a previous section, we used a local Azure SQL Edge database running in a Docker container on our droplet for the first roughly seven weeks (migration to Azure 15/3, \href{https://github.com/Chillhound/DevOps2022F/commit/64df2d7b400a116b25d448e2005b062c0fb2bf72}{commit 64df}). With this database we experienced regular attempts to brute-force the password to the superuser of the database, resulting in the Docker container with the database crashing roughly every 6-12 hours. Logs from one of the attempts is seen in appendix \ref{app:DB logs}. The downtime caused by these crashes led to missing requests from the simulator, and in general decreased the reliability and availability of the system.  We first solved the problem of attacks by not exposing our database publicly with \href{https://github.com/Chillhound/DevOps2022F/commit/2a025f54d7694fb7f59eba496befc600e8bd7546}{commit 2a02}. However, It the attacks also made us fear losing all of the data, which led to creation of (manual) backups, but having a local db was still hard to maintain and not without risk.
\\
\textbf{Migrating to the cloud} \\
For the reasons specified above we decided to migrate to a database hosted with Microsoft Azure. This increased the systems maintainability and reliabilty, as the managed db provided features such as automatic backups and security. 

%After migrating the data we had some issues with connecting the .NET project to the new database and ultimately left the connectionstring in the repository %until the start of April (\href{https://github.com/Chillhound/DevOps2022F/commit/a536ce053898e47e4da03234f9544a166bce9a62}{commit a536}, %\href{https://github.com/Chillhound/DevOps2022F/issues/55}{issue 55}).

\subsection{Handling secrets}
For the first weeks of the project, secrets was not handled correctly (database connection strings) as they were committed to our GitHub repository. \\
As described above, malicious people systematically tried to access our database and thus assuming that there is a real risk of someone scanning public repositories for secrets that can be used to exploit or damage systems, made sense. \\
Seen from another group which had gotten their data stolen because they did not password protect their database.
With the somewhat careless way we handled secrets, the same could just as well have happened to us - which again relates to the point about doing things right the first time.

\subsection{msgs endpoint becoming very slow}
Towards the end of the simulator period it was discovered that our /msgs endpoint was becoming very slow.
We identified that the way we had implemented it was ineffective, as we: 
\begin{enumerate}
    \item Sorted them according to the date they were entered 
    \item Picked the top 100 of them to show on the site
\end{enumerate}
With the amount of posts reaching almost 3 million (2.867.986) near the end, it was becoming increasingly slow. \\ 
If two of us tried to access the endpoint at the same time, the databases compute utilization would increase to around 100\% and make the system unresponsive. \\
To resolve this issue we opted to sort by the id instead, and turn on automatic indexing on the database, which drastically improved the speed of retrieval.

\section{Conclusion}
There was a great focus on incremental updates and availability as the public API was constantly used. When deploying a new update, throughout the project, the group became very aware on the logging and testing the live system status. This also resulted in doing rollbacks for availability. Given the organization and size of our team and the way we have worked together, we strived for a culture of continual experimentation, sharing of knowledge and learning \cite{devopshandbook}. It has given the freedom and trust to try out different solutions in an attempt to both gain knowledge and find the best possible solution.

Nonetheless, having the responsibility for maintenance, refactoring and evolution has given a new view of the software development process. Prior to this project nobody in the team had tried anything else than rushing to get a project done, handing it in and then not having to deal with it anymore. Here, this project showed the value of minimizing work in progress and incremental upgrades with logging, monitoring etc. which increased the observability of the system, therefore allowing for easier troubleshooting.
